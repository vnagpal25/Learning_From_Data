{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073d88c2",
   "metadata": {},
   "source": [
    "## Question 1, Answer: D\n",
    "### Problem:\n",
    "#### Recall that N is the size of the data set and d is the dimensionality of the\n",
    "#### input space. The original formulation of the hard-margin SVM problem (minimize 0.5 *\n",
    "#### wTw subject to the inequality constraints), without going through the\n",
    "#### Lagrangian dual problem, is\n",
    "### Solution:\n",
    "#### If we are trying to minimize 0.5wTw, then we are effectively trying to minimize the magnitude of w\n",
    "#### w corresponds to the d input features plus the bias. This leads to w having d + 1 parameters\n",
    "#### Hence, the quadratic programming problem has d + 1 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa758d",
   "metadata": {},
   "source": [
    "## Question 2, Answer:  A\n",
    "#### The kernel for problem 2 is of the form $K(x_n, x_m) = (1 + x_n^Tx_m)^Q$\n",
    "#### We can use the SVM classifier from the scikit-learn package and specify the required regularization constant C and kernel\n",
    "#### The SVM classifier uses a polynomial kernel of this form $K(x, x') = (\\gamma\\langle x, x' \\rangle + r)^d$, where $\\langle x, x' \\rangle$ specifies the inner product between $x$ and $x'$\n",
    "#### We can also choose $\\gamma = r= 1,d = Q$ to match our required kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99de6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm # premade library exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38b86b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading datasets using numpy\n",
    "def load_datasets():\n",
    "    train_ds = np.loadtxt('features.train')\n",
    "    test_ds = np.loadtxt('features.test')\n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "# getting a data set with labels 1 for n and -1 otherwise\n",
    "def get_masked_ds(ds, n, m=None):\n",
    "    n_mask = (ds[:, 0] == n)\n",
    "    ds[n_mask, 0] = 1\n",
    "    \n",
    "    if m:\n",
    "        m_mask = (ds[:, 0] == m)\n",
    "        ds[m_mask, 0] = -1\n",
    "        ds = ds[(n_mask | m_mask)] # removing rows that aren't either number\n",
    "    else:\n",
    "        ds[~n_mask, 0] = -1\n",
    "    \n",
    "    X = ds[:, 1:]\n",
    "    y = ds[:, 0]\n",
    "    return X, y\n",
    "\n",
    "# get X and y I/O pairs for n vs m classification\n",
    "# if m is not given, then n vs all is default assumption for classification\n",
    "def get_n_vs_m_ds(n, m=None):\n",
    "    # get train and test ds\n",
    "    train_ds, test_ds = load_datasets()\n",
    "    \n",
    "    # given the number n, mark all rows that are of the number n with 1\n",
    "    # and mark all other rows with -1\n",
    "    X_train, y_train = get_masked_ds(train_ds, n, m)\n",
    "    X_test, y_test = get_masked_ds(test_ds, n, m)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "\n",
    "# run n vs all classification SVM with specified kernels for given n values\n",
    "# return a dictionary with n as key and (E_in, num_support_vecs) as a value\n",
    "def get_n_vs_all_classification_info(n_values, C_reg, Q):\n",
    "    n_dict = {}\n",
    "    for n in n_values:\n",
    "        (X, y), _ = get_n_vs_m_ds(n)\n",
    "        classifier = svm.SVC(C=C_reg, kernel='poly', gamma=1, degree=Q, coef0=1) # getting configured SVM with polynomial kernel\n",
    "        classifier.fit(X, y)\n",
    "        y_in_predict = classifier.predict(X)\n",
    "        E_in = np.mean(y_in_predict != y)\n",
    "        n_dict[n] = (np.round(E_in, 2), classifier.support_vectors_.shape[0])   \n",
    "    return n_dict\n",
    "\n",
    "def get_n_vs_m_classification_info(n, m, C_reg_vals, Q_vals):\n",
    "    n_m_dict = {}\n",
    "    (X_tr, y_tr), (X_te, y_te) = get_n_vs_m_ds(n, m)\n",
    "    \n",
    "    for C_reg in C_reg_vals: # iterate over C\n",
    "        for Q in Q_vals: # iterate over Q\n",
    "            classifier = svm.SVC(C=C_reg, kernel='poly', gamma=1, degree=Q, coef0=1) # getting configured SVM with polynomial kernel\n",
    "            classifier.fit(X_tr, y_tr)\n",
    "\n",
    "            y_in_predict = classifier.predict(X_tr)\n",
    "            E_in = np.mean(y_in_predict != y_tr)\n",
    "\n",
    "            y_out_predict = classifier.predict(X_te)\n",
    "            E_out = np.mean(y_out_predict != y_te)\n",
    "\n",
    "            n_m_dict[(n, m, C_reg, Q)] = (np.round(E_in, 4), np.round(E_out, 4), classifier.support_vectors_.shape[0])\n",
    "    return n_m_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af6a8f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 vs all had an in sample error of 0.11 with 2179 support vectors\n",
      "2 vs all had an in sample error of 0.1 with 1970 support vectors\n",
      "4 vs all had an in sample error of 0.09 with 1856 support vectors\n",
      "6 vs all had an in sample error of 0.09 with 1893 support vectors\n",
      "8 vs all had an in sample error of 0.07 with 1776 support vectors\n",
      "\n",
      "As we can clearly see, 0 vs all had the highest in sample error of 0.11 with 2179 support vectors\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "# specifying n vs all in values for n\n",
    "# regularization constant C_reg\n",
    "# polynomial kernel degree Q\n",
    "classification_info = get_n_vs_all_classification_info(n_values=(0, 2, 4, 6, 8), C_reg=0.01, Q=2) # contains in sample error/number of support vecs for each n vs all\n",
    "for n, info in classification_info.items():\n",
    "    print(f'{n} vs all had an in sample error of {info[0]} with {info[1]} support vectors')\n",
    "print('\\nAs we can clearly see, 0 vs all had the highest in sample error of 0.11 with 2179 support vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d70e2e",
   "metadata": {},
   "source": [
    "## Question 3, Answer: A\n",
    "#### We are going to employ the same tactic as we did in problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca31ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 vs all had an in sample error of 0.01 with 386 support vectors\n",
      "3 vs all had an in sample error of 0.09 with 1950 support vectors\n",
      "5 vs all had an in sample error of 0.08 with 1585 support vectors\n",
      "7 vs all had an in sample error of 0.09 with 1704 support vectors\n",
      "9 vs all had an in sample error of 0.09 with 1978 support vectors\n",
      "\n",
      "As we can clearly see, 1 vs all had the lowest in sample error of 0.01 with 386 support vectors\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "# specifying n vs all in values for n\n",
    "# regularization constant C_reg\n",
    "# polynomial kernel degree Q\n",
    "classification_info = get_n_vs_all_classification_info(n_values=(1, 3, 5, 7, 9), C_reg=0.01, Q=2) # contains in sample error/number of support vecs for each n vs all\n",
    "for n, info in classification_info.items():\n",
    "    print(f'{n} vs all had an in sample error of {info[0]} with {info[1]} support vectors')\n",
    "print('\\nAs we can clearly see, 1 vs all had the lowest in sample error of 0.01 with 386 support vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e0abd",
   "metadata": {},
   "source": [
    "## Question 4, Answer: C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5829d831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a difference of 1793  support vectors from the selected classifiers from 1 and 2\n",
      "This is closest to answer choice C, 1800 support vectors\n"
     ]
    }
   ],
   "source": [
    "difference = 2179 - 386\n",
    "print(f'There was a difference of {difference}  support vectors from the selected classifiers from 1 and 2')\n",
    "print('This is closest to answer choice C, 1800 support vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd97981",
   "metadata": {},
   "source": [
    "## Question 5, Answer: D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "345e5695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using regularization constant C=0.001 had in sample error 0.0045 and out of sample error 0.0165 with 76 support vectors\n",
      "\n",
      "Using regularization constant C=0.01 had in sample error 0.0045 and out of sample error 0.0189 with 34 support vectors\n",
      "\n",
      "Using regularization constant C=0.1 had in sample error 0.0045 and out of sample error 0.0189 with 24 support vectors\n",
      "\n",
      "Using regularization constant C=1 had in sample error 0.0032 and out of sample error 0.0189 with 24 support vectors\n",
      "\n",
      "\n",
      "\n",
      "We can clearly see a maximum C=1 achieves the lowest E_in = 0.0032. This means that Answer D is correct\n"
     ]
    }
   ],
   "source": [
    "classification_info = get_n_vs_m_classification_info(n=1, m=5, C_reg_vals=[0.001, 0.01, 0.1, 1], Q_vals=[2])\n",
    "for key, value in classification_info.items():\n",
    "    _, _, C_reg, _ = key\n",
    "    E_in, E_out, num_support_vectors = value\n",
    "    print(f'Using regularization constant C={C_reg} had in sample error {E_in} and out of sample error {E_out} with {num_support_vectors} support vectors\\n')\n",
    "    \n",
    "print('\\n\\nWe can clearly see a maximum C=1 achieves the lowest E_in = 0.0032. This means that Answer D is correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd354b7e",
   "metadata": {},
   "source": [
    "## Question 6, Answer: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
