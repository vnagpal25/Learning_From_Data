{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75aa6d04",
   "metadata": {},
   "source": [
    "## Question 1, Answer: D\n",
    "### Problem:\n",
    "#### Recall that N is the size of the data set and d is the dimensionality of the\n",
    "#### input space. The original formulation of the hard-margin SVM problem (minimize 0.5 *\n",
    "#### wTw subject to the inequality constraints), without going through the\n",
    "#### Lagrangian dual problem, is\n",
    "### Solution:\n",
    "#### If we are trying to minimize 0.5wTw, then we are effectively trying to minimize the magnitude of w\n",
    "#### w corresponds to the d input features plus the bias. This leads to w having d + 1 parameters\n",
    "#### Hence, the quadratic programming problem has d + 1 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb6995",
   "metadata": {},
   "source": [
    "## Question 2, Answer:  A\n",
    "#### The kernel for problem 2 is of the form $K(x_n, x_m) = (1 + x_n^Tx_m)^Q$\n",
    "#### We can use the SVM classifier from the scikit-learn package and specify the required regularization constant C and kernel\n",
    "#### The SVM classifier uses a polynomial kernel of this form $K(x, x') = (\\gamma\\langle x, x' \\rangle + r)^d$, where $\\langle x, x' \\rangle$ specifies the inner product between $x$ and $x'$\n",
    "#### We can also choose $\\gamma = r= 1,d = Q$ to match our required kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed7370cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from sklearn import svm, model_selection\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e405c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading datasets using numpy\n",
    "def load_datasets():\n",
    "    train_ds = np.loadtxt('features.train')\n",
    "    test_ds = np.loadtxt('features.test')\n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "# getting a data set with labels 1 for n and -1 otherwise\n",
    "def get_masked_ds(ds, n, m=None):\n",
    "    # mark all rows with n with 1\n",
    "    n_mask = (ds[:, 0] == n)\n",
    "    ds[n_mask, 0] = 1\n",
    "    \n",
    "    # -1 labels\n",
    "    if m:\n",
    "        # only label rows with m as -1\n",
    "        m_mask = (ds[:, 0] == m)\n",
    "        ds[m_mask, 0] = -1\n",
    "        \n",
    "         # discarding rows that aren't either m or n\n",
    "        ds = ds[(n_mask | m_mask)]\n",
    "    else:\n",
    "        # mark all other rows as -1\n",
    "        ds[~n_mask, 0] = -1\n",
    "    \n",
    "    # returning X(features), y(labels)\n",
    "    X = ds[:, 1:]\n",
    "    y = ds[:, 0]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# get X and y I/O pairs for n vs m classification\n",
    "# if m is not given, then n vs all is default assumption for classification\n",
    "def get_n_vs_m_ds(n, m=None):\n",
    "    # get train and test ds\n",
    "    train_ds, test_ds = load_datasets()\n",
    "    \n",
    "    # given the number n, mark all rows that are of the number n with 1\n",
    "    # and mark all other rows with -1\n",
    "    X_train, y_train = get_masked_ds(train_ds, n, m)\n",
    "    X_test, y_test = get_masked_ds(test_ds, n, m)\n",
    "    \n",
    "    # returns\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "\n",
    "# run n vs all classification SVM with specified kernels for given n values\n",
    "# return a dictionary with n as key and (E_in, num_support_vecs) as a value\n",
    "def get_n_vs_all_classification_info(n_values, C_reg, Q):\n",
    "    n_dict = {}\n",
    "    for n in n_values: # iterates over different n\n",
    "        # gets dataset\n",
    "        (X, y), _ = get_n_vs_m_ds(n)\n",
    "        \n",
    "        # fits the data\n",
    "        classifier = svm.SVC(C=C_reg, kernel='poly', gamma=1, degree=Q, coef0=1) # getting configured SVM with polynomial kernel\n",
    "        classifier.fit(X, y)\n",
    "        \n",
    "        # evaluating in sample error\n",
    "        y_in_predict = classifier.predict(X)\n",
    "        E_in = np.mean(y_in_predict != y)\n",
    "        \n",
    "        # to return result\n",
    "        n_dict[n] = (np.round(E_in, 2), classifier.support_vectors_.shape[0])   \n",
    "    return n_dict\n",
    "\n",
    "\n",
    "# evaluates classification of polynomial kernel for n vs m classification for varying degrees of kernel, and varying regularization constants\n",
    "def get_n_vs_m_classification_info(n, m, C_reg_vals, Q_vals):\n",
    "    n_m_dict = {}\n",
    "    \n",
    "    # getting datasets\n",
    "    (X_tr, y_tr), (X_te, y_te) = get_n_vs_m_ds(n, m)\n",
    "    \n",
    "    for C_reg in C_reg_vals: # iterate over C\n",
    "        for Q in Q_vals: # iterate over Q\n",
    "            # fitting the data\n",
    "            classifier = svm.SVC(C=C_reg, kernel='poly', gamma=1, degree=Q, coef0=1) # getting configured SVM with polynomial kernel\n",
    "            classifier.fit(X_tr, y_tr)\n",
    "            \n",
    "            # in sample error calc\n",
    "            y_in_predict = classifier.predict(X_tr)\n",
    "            E_in = np.mean(y_in_predict != y_tr)\n",
    "            \n",
    "            # out of sample error calc\n",
    "            y_out_predict = classifier.predict(X_te)\n",
    "            E_out = np.mean(y_out_predict != y_te)\n",
    "            \n",
    "            # to return result\n",
    "            n_m_dict[(n, m, C_reg, Q)] = (np.round(E_in, 4), np.round(E_out, 4), classifier.support_vectors_.shape[0])\n",
    "    return n_m_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09ddeb77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 vs all had an in sample error of 0.11 with 2179 support vectors\n",
      "2 vs all had an in sample error of 0.1 with 1970 support vectors\n",
      "4 vs all had an in sample error of 0.09 with 1856 support vectors\n",
      "6 vs all had an in sample error of 0.09 with 1893 support vectors\n",
      "8 vs all had an in sample error of 0.07 with 1776 support vectors\n",
      "\n",
      "As we can clearly see, 0 vs all had the highest in sample error of 0.11 with 2179 support vectors\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "# specifying n vs all in values for n\n",
    "# regularization constant C_reg\n",
    "# polynomial kernel degree Q\n",
    "classification_info = get_n_vs_all_classification_info(n_values=(0, 2, 4, 6, 8), C_reg=0.01, Q=2) # contains in sample error/number of support vecs for each n vs all\n",
    "for n, info in classification_info.items():\n",
    "    print(f'{n} vs all had an in sample error of {info[0]} with {info[1]} support vectors')\n",
    "print('\\nAs we can clearly see, 0 vs all had the highest in sample error of 0.11 with 2179 support vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc10a5",
   "metadata": {},
   "source": [
    "## Question 3, Answer: A\n",
    "#### We are going to employ the same tactic as we did in problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6684896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 vs all had an in sample error of 0.01 with 386 support vectors\n",
      "3 vs all had an in sample error of 0.09 with 1950 support vectors\n",
      "5 vs all had an in sample error of 0.08 with 1585 support vectors\n",
      "7 vs all had an in sample error of 0.09 with 1704 support vectors\n",
      "9 vs all had an in sample error of 0.09 with 1978 support vectors\n",
      "\n",
      "As we can clearly see, 1 vs all had the lowest in sample error of 0.01 with 386 support vectors\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "# specifying n vs all in values for n\n",
    "# regularization constant C_reg\n",
    "# polynomial kernel degree Q\n",
    "classification_info = get_n_vs_all_classification_info(n_values=(1, 3, 5, 7, 9), C_reg=0.01, Q=2) # contains in sample error/number of support vecs for each n vs all\n",
    "for n, info in classification_info.items():\n",
    "    print(f'{n} vs all had an in sample error of {info[0]} with {info[1]} support vectors')\n",
    "print('\\nAs we can clearly see, 1 vs all had the lowest in sample error of 0.01 with 386 support vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0b54d",
   "metadata": {},
   "source": [
    "## Question 4, Answer: C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44a8df14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a difference of 1793  support vectors from the selected classifiers from 1 and 2\n",
      "This is closest to answer choice C, 1800 support vectors\n"
     ]
    }
   ],
   "source": [
    "difference = 2179 - 386\n",
    "print(f'There was a difference of {difference}  support vectors from the selected classifiers from 1 and 2')\n",
    "print('This is closest to answer choice C, 1800 support vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb523e0",
   "metadata": {},
   "source": [
    "## Question 5, Answer: D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1d4e8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using regularization constant C=0.001 had in sample error 0.0045 and out of sample error 0.0165 with 76 support vectors\n",
      "\n",
      "Using regularization constant C=0.01 had in sample error 0.0045 and out of sample error 0.0189 with 34 support vectors\n",
      "\n",
      "Using regularization constant C=0.1 had in sample error 0.0045 and out of sample error 0.0189 with 24 support vectors\n",
      "\n",
      "Using regularization constant C=1 had in sample error 0.0032 and out of sample error 0.0189 with 24 support vectors\n",
      "\n",
      "\n",
      "\n",
      "We can clearly see a maximum C=1 achieves the lowest E_in = 0.0032. This means that Answer D is correct\n"
     ]
    }
   ],
   "source": [
    "classification_info = get_n_vs_m_classification_info(n=1, m=5, C_reg_vals=[0.001, 0.01, 0.1, 1], Q_vals=[2])\n",
    "for key, value in classification_info.items():\n",
    "    _, _, C_reg, _ = key\n",
    "    E_in, E_out, num_support_vectors = value\n",
    "    print(f'Using regularization constant C={C_reg} had in sample error {E_in} and out of sample error {E_out} with {num_support_vectors} support vectors\\n')\n",
    "    \n",
    "print('\\n\\nWe can clearly see a maximum C=1 achieves the lowest E_in = 0.0032. This means that Answer D is correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812690f",
   "metadata": {},
   "source": [
    "## Question 6, Answer: B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "264b6829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using regularization constant C=0.0001 and kernel degree Q =2 had in sample error 0.009 and out of sample error 0.0165 with 236 support vectors\n",
      "\n",
      "\n",
      "Using regularization constant C=0.0001 and kernel degree Q =5 had in sample error 0.0045 and out of sample error 0.0189 with 26 support vectors\n",
      "\n",
      "\n",
      "Using regularization constant C=0.001 and kernel degree Q =2 had in sample error 0.0045 and out of sample error 0.0165 with 76 support vectors\n",
      "\n",
      "\n",
      "Using regularization constant C=0.001 and kernel degree Q =5 had in sample error 0.0045 and out of sample error 0.0212 with 25 support vectors\n",
      "\n",
      "\n",
      "Using regularization constant C=0.01 and kernel degree Q =2 had in sample error 0.0045 and out of sample error 0.0189 with 34 support vectors\n",
      "\n",
      "\n",
      "Using regularization constant C=0.01 and kernel degree Q =5 had in sample error 0.0038 and out of sample error 0.0212 with 23 support vectors\n",
      "\n",
      "\n",
      "Using regularization constant C=1 and kernel degree Q =2 had in sample error 0.0032 and out of sample error 0.0189 with 24 support vectors\n",
      "\n",
      "\n",
      "Using regularization constant C=1 and kernel degree Q =5 had in sample error 0.0032 and out of sample error 0.0212 with 21 support vectors\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We can clearly see that when C=0.001, the number of support vectors is lower when Q = 5. This means that Answer B is correct\n"
     ]
    }
   ],
   "source": [
    "classification_info = get_n_vs_m_classification_info(n=1, m=5, C_reg_vals=[0.0001, 0.001, 0.01, 1], Q_vals=[2, 5])\n",
    "for key, value in classification_info.items():\n",
    "    _, _, C_reg, Q = key\n",
    "    E_in, E_out, num_support_vectors = value\n",
    "    print(f'Using regularization constant C={C_reg} and kernel degree Q ={Q} had in sample error {E_in} and out of sample error {E_out} with {num_support_vectors} support vectors\\n\\n')\n",
    "    \n",
    "print('\\n\\nWe can clearly see that when C=0.001, the number of support vectors is lower when Q = 5. This means that Answer B is correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c7af4",
   "metadata": {},
   "source": [
    "## Question 7, Answer: B\n",
    "## Question 8, Answer: C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c55c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate E_cv by randomly partitioning training dataset into 10 portions\n",
    "# do 10 seperate training sessions, choosing 1 of the portions as the validation set\n",
    "# calculate cross validation error as average of 10 validation errors\n",
    "def cross_validation_model_selection(n, m, C_vals=[0.0001, 0.001, 0.01, 0.1, 1], Q=2):\n",
    "    # 10 fold data splitting indices\n",
    "    k_fold = model_selection.StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    \n",
    "    # getting dataset, only training\n",
    "    (X_tr, y_tr), _ = get_n_vs_m_ds(n, m)\n",
    "    \n",
    "    E_vals_per_C = {}\n",
    "    \n",
    "    # training on 10 uniquely different partitions of data\n",
    "    for train_index, test_index in k_fold.split(X_tr, y_tr):\n",
    "        for C_reg in C_vals:\n",
    "            X_train, X_val = X_tr[train_index], X_tr[test_index]\n",
    "            y_train, y_val = y_tr[train_index], y_tr[test_index]\n",
    "\n",
    "            classifier = svm.SVC(C=C_reg, kernel='poly', gamma=1, degree=Q, coef0=1) # getting configured SVM with polynomial kernel\n",
    "            classifier.fit(X_train, y_train)\n",
    "\n",
    "            y_val_predict = classifier.predict(X_val)\n",
    "            E_val = np.mean(y_val_predict != y_val)\n",
    "\n",
    "            if C_reg not in E_vals_per_C:\n",
    "                E_vals_per_C[C_reg] = [E_val]\n",
    "            else:\n",
    "                E_vals_per_C[C_reg].append(E_val)\n",
    "        \n",
    "    # calculating cross validation error for each C\n",
    "    E_cv_per_C = {}\n",
    "    for C_reg, E_vals in E_vals_per_C.items():\n",
    "        E_cv_per_C[C_reg] = np.mean(E_vals)\n",
    "    \n",
    "    # finding the C with the minimum cross validation error and returning it\n",
    "    min_E_cv = min(E_cv_per_C.values())\n",
    "    for C in C_vals:\n",
    "        if E_cv_per_C[C] == min_E_cv:\n",
    "            return (C, min_E_cv)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0002a56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most commonly selected regularization parameter was C=0.001 with an average cross validation error of E_cv=0.0046\n",
      "This points us to answer choice B for question 7 and answer choice C for question 8\n"
     ]
    }
   ],
   "source": [
    "selected_Cs = []\n",
    "E_cv_vals = {}\n",
    "\n",
    "# performing experiment 100 times\n",
    "for i in range(100):\n",
    "    C_selected, E_cv = cross_validation_model_selection(1, 5) # for 1 vs 5\n",
    "    selected_Cs.append(C_selected)\n",
    "    if C_selected not in E_cv_vals:\n",
    "        E_cv_vals[C_selected] = [E_cv]\n",
    "    else:\n",
    "        E_cv_vals[C_selected].append(E_cv)\n",
    "\n",
    "# finding most selected C and computing the average cross validation error for it\n",
    "most_selected_C = mode(selected_Cs)\n",
    "E_cv_avg = np.round(np.mean(E_cv_vals[most_selected_C]), 4)\n",
    "print(f'The most commonly selected regularization parameter was C={most_selected_C} with an average cross validation error of E_cv={E_cv_avg}')\n",
    "print(f'This points us to answer choice B for question 7 and answer choice C for question 8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484574cc",
   "metadata": {},
   "source": [
    "## Question 9, Answer: E\n",
    "## Question 10, Answer: C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "06d4ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns classification performance for n vs m for RBF kernel choice for varying regularization constants\n",
    "def get_n_vs_m_classification_info_rbf(n, m, C_reg_vals=(0.01, 1, 100, 10**4, 10**6)):\n",
    "    n_m_dict = {}\n",
    "    (X_tr, y_tr), (X_te, y_te) = get_n_vs_m_ds(n, m) # dataset\n",
    "    \n",
    "    for C_reg in C_reg_vals: # iterate over C vals\n",
    "            # fitting data\n",
    "            classifier = svm.SVC(C=C_reg, kernel='rbf', gamma=1) # getting configured SVM with rbf kernel\n",
    "            classifier.fit(X_tr, y_tr)\n",
    "            \n",
    "            # in sample error\n",
    "            y_in_predict = classifier.predict(X_tr)\n",
    "            E_in = np.mean(y_in_predict != y_tr)\n",
    "\n",
    "            # out of sample error\n",
    "            y_out_predict = classifier.predict(X_te)\n",
    "            E_out = np.mean(y_out_predict != y_te)\n",
    "            \n",
    "            # to return result\n",
    "            n_m_dict[C_reg] = (np.round(E_in, 4), np.round(E_out, 4), classifier.support_vectors_.shape[0])\n",
    "    return n_m_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "863a955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C_reg=0.01, E_in=0.0038, E_out = 0.0236\n",
      "\n",
      "For C_reg=1, E_in=0.0045, E_out = 0.0212\n",
      "\n",
      "For C_reg=100, E_in=0.0032, E_out = 0.0189\n",
      "\n",
      "For C_reg=10000, E_in=0.0026, E_out = 0.0236\n",
      "\n",
      "For C_reg=1000000, E_in=0.0006, E_out = 0.0236\n",
      "\n",
      "As we can see the minimum in sample error E_in is achieved for C = 10^6.\n",
      " This points us to answer choice E for question 9\n",
      "As we can see the minimum out of sample error E_out is achieved for C = 100. This points us to answer choice C for question 10\n"
     ]
    }
   ],
   "source": [
    "rbf_classifciation_info = get_n_vs_m_classification_info_rbf(1, 5)\n",
    "for C_reg, value in rbf_classifciation_info.items():\n",
    "    print(f'For C_reg={C_reg}, E_in={value[0]}, E_out = {value[1]}\\n')\n",
    "print('As we can see the minimum in sample error E_in is achieved for C = 10^6.\\n This points us to answer choice E for question 9')\n",
    "print('As we can see the minimum out of sample error E_out is achieved for C = 100. This points us to answer choice C for question 10')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
