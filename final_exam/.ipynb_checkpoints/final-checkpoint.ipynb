{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de3e653",
   "metadata": {},
   "source": [
    "# Question 1, Answer E\n",
    "## A non linear polynomial transformation $\\phi$ of order $n$ applied to a an input space $X$ with dimension $d=2$ looks like the following\n",
    "## $x = (1, x_1, x_2)$\n",
    "## $\\phi(x) = (1, x_1, x_2, x_1^2, x_1x_2,x_2^2, ..., x_1^n, x_1^{n-1}x_2, ..., x_1x_2^{n-1}, x_2^n)$\n",
    "## To find the dimension of the image of $x$ under the non-linear transformatin, it suffices to count the elements in the d-tuple, excluding the first element 1\n",
    "## First of all, it is necessary to note that for any non-negative integer $m$, there are exactly $m + 1$ possible terms of order $m$ given two variables $x_1, x_2$.\n",
    "## To see this, let us enumerate all terms:\n",
    "## $x_1^mx_2^0, x_1^{m-1}x_2^1, ..., x_1^1x_2^{m-1}, x_1^0x_2^m$\n",
    "## In the above sequence of $n$-order terms, there are $m+1$ powers of $x_2$(or $x_1$), ranging from $0$ to $m$.\n",
    "## For a transform of order $n$, we must count all possible terms with powers ranging from $1$ to $n$(remember we are excluding power $0$, which is the first term in the transform)\n",
    "## This can thus be represented as a succinct sum as the following\n",
    "## $d = \\sum_{m=1}^n{m+1}$\n",
    "## This sum can be computed as: $d(n) = \\sum_{m=1}^n{(m+1)} = \\sum_{m=1}^n{m} + \\sum_{m=1}^n{1} = \\sum_{m=1}^n{m} + n = \\frac{n(n+1)}{2} + n = \\frac{n(n+3)}{2}$\n",
    "## Therefore, for a polynomial transform of order $10$, we can compute the dimension of the image of a $2$-dimensional input as $d(10) = \\frac{10(13)}{2} = 65$ \n",
    "## Hence, the correct answer is none of the above, E."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0ed85",
   "metadata": {},
   "source": [
    "# Question 2, Answer D\n",
    "## The expected value of a set $\\{g^{(D)}\\}$ is an average of all of the hypotheses. To illustrate that this average is not contained within $H$, let us consider two hypotheses $h_1, h_2 \\epsilon H$.\n",
    "\n",
    "## Using logistic regression, $h_1, h_2$ are of the form:\n",
    "## $h_1 = \\theta_1(x) = \\frac{1}{1 + e^{-w_1^Tx}}, h_2 = \\theta_2(x) = \\frac{1}{1 + e^{-w_2^Tx}}$\n",
    "## Their average can be written, in short after some algebraic manipulation, as follows:\n",
    "## $\\frac{h_1 + h_2}{2} = \\frac{2 + e^{-w_1^Tx} + e^{-w_2^Tx}}{2(1 + e^{-w_1^Tx} + e^{-w_2^Tx} + e^{-(w_1 + w_2)^Tx})}$\n",
    "## This hypothesis is not of the form of either h_1 and h_2\n",
    "## Therefore, answer D is the correct answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba4df5",
   "metadata": {},
   "source": [
    "# Question 3, Answer D\n",
    "## Choice D: We can always determine if there is overfitting by comparing the values of $(E_{out} - E_{in})$\n",
    "## There might be two hypotheses, $h_1$ with $E_{in} = 0.2, E_{out} = 0.6, E_{out}-E_{in} = 0.4$ and $h_2$ with $E_{in} = 0.5, E_{out} = 0.7, E_{out}-E_{in} = 0.2$\n",
    "## Even though $h_1$ has a higher value for $E_{out}-E_{in}$, it might initially be a candidate for overfitting, upon closer inspection, we know this to be false. It actually has a lower E_out than $h_2$ and therefore is not overfitting.\n",
    "\n",
    "## Therefore, the correct answer choice is D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea2e077",
   "metadata": {},
   "source": [
    "# Question 4, Answer D\n",
    "## Stochastic noise depends on the output distribution of the target function, and not on the ability of the hypothesis set to approximate the function, which constitutes deterministic noise\n",
    "## Therefore, the correct answer chocie is D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8d610",
   "metadata": {},
   "source": [
    "# Question 5, Answer A\n",
    "## If the solution from linear regression satisfies the regularization constraint, then regularization imposes no constraint on the solution.\n",
    "## The same solution as unconstrained linear regression is returned\n",
    "## Therefore, the correct answer choice is A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3123ed0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Question 6, Answer B\n",
    "## The soft order constraint attempts to minimize the error subject to a certain constraint\n",
    "## The augmented error formulation attemps to minimize the augmented error (the in sample error with an added term)\n",
    "## Mathematically, these are both identical. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398cfaa",
   "metadata": {},
   "source": [
    "# Question 7, Answer D\n",
    "# Question 8, Answer B\n",
    "## Recall that the solution for linear regression without validation is: $w_{lin} = (Z^TZ)^{-1}Z^Ty$\n",
    "## The solution that includes regularization is very similar and just has an added term within the pseudoinverse: $w_{reg} = (Z^TZ + \\lambda I)^{-1}Z^Ty$, where $\\lambda$ is the regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c4cb1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "# train_file = wget.download('http://www.amlbook.com/data/zip/features.train')\n",
    "# test_file = wget.download('http://www.amlbook.com/data/zip/features.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c71f0d47-7036-4ebe-94e6-a3c7d7ba76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"getting files from Caltech server using wget, loading datasets using NumPy\"\"\"\n",
    "    train_ds = np.loadtxt(train_file)\n",
    "    test_ds = np.loadtxt(test_file)\n",
    "    return train_ds, test_ds\n",
    "    \n",
    "def get_masked_ds(ds, n, m=None):\n",
    "    \"\"\"getting a data set with labels 1 for n and -1 otherwise\"\"\"\n",
    "    \n",
    "    # mark all rows with n with 1\n",
    "    n_mask = (ds[:, 0] == n)\n",
    "    ds[n_mask, 0] = 1\n",
    "    \n",
    "    # -1 labels\n",
    "    if m:\n",
    "        # Only label rows with m as -1\n",
    "        m_mask = (ds[:, 0] == m)\n",
    "        ds[m_mask, 0] = -1\n",
    "        \n",
    "         # discarding rows that aren't either m or n\n",
    "        ds = ds[(n_mask | m_mask)]\n",
    "    else:\n",
    "        # mark all other rows as -1\n",
    "        ds[~n_mask, 0] = -1\n",
    "\n",
    "    # removing digit label and adding 1 as bias coordinate to each row\n",
    "    X = np.insert(ds[:, 1:], 0, 1, axis=1)    \n",
    "    y = ds[:, 0]\n",
    "    \n",
    "    # returning X(features), y(labels)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def get_n_vs_m_ds(n, m=None):\n",
    "    \"\"\"# get X and y I/O pairs for n vs m classification\n",
    "        # if m is not given, then n vs all is default assumption for classification\"\"\"\n",
    "    # get train and test ds\n",
    "    train_ds, test_ds = load_data()\n",
    "    \n",
    "    # given the number n, mark all rows that are of the number n with 1\n",
    "    # and mark all other rows with -1\n",
    "    X_train, y_train = get_masked_ds(train_ds, n, m)\n",
    "    X_test, y_test = get_masked_ds(test_ds, n, m)\n",
    "    \n",
    "    # returns\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "\n",
    "def run_linear_regression(Z, y, lambda_reg):\n",
    "    \"\"\"Given input features Z, output class y, and regularization parameter lambda, return the result of regularized regression\"\"\"\n",
    "    _, cols = np.shape(Z)\n",
    "    ZtZ = np.dot(Z.T, Z)\n",
    "    pinv_Z = np.dot(np.linalg.inv(ZtZ + lambda_reg * np.identity(cols)), Z.T)\n",
    "    w = np.dot(pinv_Z, y) # w = pseudo_inv(X)*y \n",
    "    return w      \n",
    "    \n",
    "def compute_error(w, Z, y):\n",
    "    \"\"\"#for each 'point'/row z in Z, compute wTz and compare sign(wTz) with y\n",
    "     # will contain 0 or 1, for correct/incorrect classification\"\"\"\n",
    "    error_list = [(np.sign(w.T @ z) != y_point) for z, y_point in zip(Z, y)]\n",
    "    return np.sum(error_list)/len(error_list)\n",
    "\n",
    "def transform(X):\n",
    "    Z = np.array([(1, point[1], point[2], point[1]*point[2], point[1] ** 2, point[2] ** 2) for point in X])\n",
    "    return Z\n",
    "    \n",
    "def lin_reg_regularized_n_vs_all(n_list, lambda_reg, apply_transform=False):\n",
    "    \"\"\"Applies linear regression to solve n vs all classification\"\"\"\n",
    "    error_dict = {}\n",
    "    for n in n_list:\n",
    "        (Z_train, y_train), (Z_test, y_test) =  get_n_vs_m_ds(n)\n",
    "\n",
    "        if apply_transform:\n",
    "            Z_train = transform(Z_train)\n",
    "            Z_test = transform(Z_test)\n",
    "        \n",
    "        w_reg = run_linear_regression(Z_train, y_train, lambda_reg)\n",
    "        E_in = compute_error(w_reg, Z_train, y_train)\n",
    "        E_out = compute_error(w_reg, Z_test, y_test)    \n",
    "        error_dict[n] = (E_in, E_out)\n",
    "    return error_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ce29b52-6204-4531-9d72-740afa7a0693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 vs all classification using regularized regression had E_in=0.07625840076807022\n",
      "6 vs all classification using regularized regression had E_in=0.09107118365107666\n",
      "7 vs all classification using regularized regression had E_in=0.08846523110684405\n",
      "8 vs all classification using regularized regression had E_in=0.07433822520916199\n",
      "9 vs all classification using regularized regression had E_in=0.08832807570977919\n",
      "As we can see, 8 vs all had the lowest in-sample error, therefore the correct answer choice is D\n"
     ]
    }
   ],
   "source": [
    "lin_reg_results = lin_reg_regularized_n_vs_all(n_list=[5, 6, 7, 8, 9], lambda_reg=1)\n",
    "for n, errors in lin_reg_results.items():\n",
    "    print(f'{n} vs all classification using regularized regression had E_in={errors[0]}')\n",
    "print('As we can see, 8 vs all had the lowest in-sample error, therefore the correct answer choice is D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d182bab-8f2c-423b-9fe5-f16acb0b55c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 vs all classification using regularized regression had E_out=0.10662680617837568\n",
      "1 vs all classification using regularized regression had E_out=0.02192326856003986\n",
      "2 vs all classification using regularized regression had E_out=0.09865470852017937\n",
      "3 vs all classification using regularized regression had E_out=0.08271051320378675\n",
      "4 vs all classification using regularized regression had E_out=0.09965122072745392\n",
      "As we can see, 1 vs all had the lowest out of sample error, therefore the correct answer choice is B\n"
     ]
    }
   ],
   "source": [
    "lin_reg_results = lin_reg_regularized_n_vs_all(n_list=[0, 1, 2, 3, 4], lambda_reg=1, apply_transform=True)\n",
    "for n, errors in lin_reg_results.items():\n",
    "    print(f'{n} vs all classification using regularized regression had E_out={errors[1]}')\n",
    "print('As we can see, 1 vs all had the lowest out of sample error, therefore the correct answer choice is B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a8ef8-1bed-48fd-aaa2-c04bacbf00d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Question 9, Answer E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bf76473-4c7a-42dc-af68-621550543854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_regularized_n_vs_all_transform(n_list, lambda_reg):\n",
    "    \"\"\"Applies linear regression to solve n vs all classification\"\"\"\n",
    "    error_dict = {}\n",
    "    for n in n_list:\n",
    "        (X_train, y_train), (X_test, y_test) =  get_n_vs_m_ds(n)\n",
    "    \n",
    "        Z_train = transform(X_train)\n",
    "        Z_test = transform(X_test)\n",
    "        \n",
    "        w_reg = run_linear_regression(X_train, y_train, lambda_reg)\n",
    "        \n",
    "        w_reg_trans = run_linear_regression(Z_train, y_train, lambda_reg)\n",
    "        \n",
    "        E_in_no_trans = compute_error(w_reg, X_train, y_train)\n",
    "        E_out_no_trans = compute_error(w_reg, X_test, y_test)\n",
    "\n",
    "        E_in_trans = compute_error(w_reg_trans, Z_train, y_train)\n",
    "        E_out_trans = compute_error(w_reg_trans, Z_test, y_test)\n",
    "        \n",
    "        error_dict[n] = (E_in_no_trans, E_out_no_trans, E_in_trans, E_out_trans)\n",
    "    return error_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5bae3394-5b40-4184-be8e-e6254d239d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 vs all classification using regularized regression had:\n",
      "Without transform: E_in =0.10931 and E_out=0.1151\n",
      "With transform: E_in =0.10232 and E_out=0.10663\n",
      "E_in_difference = 0.00699, E_out_difference=-0.00847\n",
      "\n",
      "\n",
      "better out of sample performance after transformation\n",
      "1 vs all classification using regularized regression had:\n",
      "Without transform: E_in =0.01522 and E_out=0.02242\n",
      "With transform: E_in =0.01234 and E_out=0.02192\n",
      "E_in_difference = 0.00288, E_out_difference=-0.0005\n",
      "\n",
      "\n",
      "2 vs all classification using regularized regression had:\n",
      "Without transform: E_in =0.10026 and E_out=0.09865\n",
      "With transform: E_in =0.10026 and E_out=0.09865\n",
      "E_in_difference = 0.0, E_out_difference=0.0\n",
      "\n",
      "\n",
      "3 vs all classification using regularized regression had:\n",
      "Without transform: E_in =0.09025 and E_out=0.08271\n",
      "With transform: E_in =0.09025 and E_out=0.08271\n",
      "E_in_difference = 0.0, E_out_difference=0.0\n",
      "\n",
      "\n",
      "4 vs all classification using regularized regression had:\n",
      "Without transform: E_in =0.08943 and E_out=0.09965\n",
      "With transform: E_in =0.08943 and E_out=0.09965\n",
      "E_in_difference = 0.0, E_out_difference=0.0\n",
      "\n",
      "\n",
      "5 vs all classification using regularized regression had:\n",
      "Without transform: E_in =0.07626 and E_out=0.07972\n",
      "With transform: E_in =0.07626 and E_out=0.07922\n",
      "E_in_difference = 0.0, E_out_difference=-0.0005\n",
      "\n",
      "\n",
      "6 vs all classification using regularized regression had:\n",
      "Without transform: E_in =0.09107 and E_out=0.0847\n",
      "With transform: E_in =0.09107 and E_out=0.0847\n",
      "E_in_difference = 0.0, E_out_difference=0.0\n",
      "\n",
      "\n",
      "7 vs all classification using regularized regression had:\n",
      "Without transform: E_in =0.08847 and E_out=0.07324\n",
      "With transform: E_in =0.08847 and E_out=0.07324\n",
      "E_in_difference = 0.0, E_out_difference=0.0\n",
      "\n",
      "\n",
      "8 vs all classification using regularized regression had:\n",
      "Without transform: E_in =0.07434 and E_out=0.08271\n",
      "With transform: E_in =0.07434 and E_out=0.08271\n",
      "E_in_difference = 0.0, E_out_difference=0.0\n",
      "\n",
      "\n",
      "9 vs all classification using regularized regression had:\n",
      "Without transform: E_in =0.08833 and E_out=0.08819\n",
      "With transform: E_in =0.08833 and E_out=0.08819\n",
      "E_in_difference = 0.0, E_out_difference=0.0\n",
      "\n",
      "\n",
      "As we can see, out of sample performance only improves for 5 vs all, but only marginally. Therefore, the correct answer is E\n"
     ]
    }
   ],
   "source": [
    "lin_reg_results = lin_reg_regularized_n_vs_all_transform(n_list=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], lambda_reg=1)\n",
    "for n, errors in lin_reg_results.items():\n",
    "    E_in_no_trans, E_out_no_trans, E_in_trans, E_out_trans = errors\n",
    "    print(f'{n} vs all classification using regularized regression had:\\n'\n",
    "          f'Without transform: E_in ={np.round(E_in_no_trans, 5)} and E_out={np.round(E_out_no_trans, 5)}\\n'\n",
    "          f'With transform: E_in ={np.round(E_in_trans, 5)} and E_out={np.round(E_out_trans, 5)}\\n'\n",
    "          f'E_in_difference = {np.round(E_in_no_trans - E_in_trans,5)}, E_out_difference={np.round(E_out_trans - E_out_no_trans, 5)}\\n\\n')\n",
    "    if E_out_trans <= 0.95*E_out_no_trans:\n",
    "        print(\"better out of sample performance after transformation\")\n",
    "print('As we can see, out of sample performance only improves for 5 vs all, but only marginally. Therefore, the correct answer is E')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d0963-9ff0-4b1a-8172-1c53c6266960",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Question 10, Answer A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d96f7408-20e2-4e44-95c8-d6ece9971aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_regularized_n_vs_m_transform(n, m, lambda_reg_vals):\n",
    "    \"\"\"Applies linear regression to solve n vs all classification\"\"\"\n",
    "    error_dict = {}\n",
    "    for lambda_reg in lambda_reg_vals:\n",
    "        (X_train, y_train), (X_test, y_test) =  get_n_vs_m_ds(n, m)\n",
    "    \n",
    "        Z_train = transform(X_train)\n",
    "        Z_test = transform(X_test)\n",
    "                \n",
    "        w_reg_trans = run_linear_regression(Z_train, y_train, lambda_reg)\n",
    "        \n",
    "        E_in = compute_error(w_reg_trans, Z_train, y_train)\n",
    "        E_out = compute_error(w_reg_trans, Z_test, y_test)\n",
    "        \n",
    "        error_dict[(n, m, lambda_reg)] = (E_in, E_out)\n",
    "    return error_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "13637927-85c0-4c25-b80e-84740d9bf0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 vs 5 classification using regularized regression with lambda=0.01 had E_in=0.004484304932735426 and E_out=0.02830188679245283\n",
      "1 vs 5 classification using regularized regression with lambda=1 had E_in=0.005124919923126201 and E_out=0.025943396226415096\n",
      "As we can see, when we decrease lambda, our E_in decreases and our E_out increases. This constitutes overfitting, so the correct answer is A\n"
     ]
    }
   ],
   "source": [
    "lin_reg_results = lin_reg_regularized_n_vs_m_transform(n=1, m=5, lambda_reg_vals=[0.01, 1])\n",
    "for (n, m, lambda_reg), (E_in, E_out) in lin_reg_results.items():\n",
    "    print(f'{n} vs {m} classification using regularized regression with lambda={lambda_reg} had E_in={E_in} and E_out={E_out}')\n",
    "print('As we can see, when we decrease lambda, our E_in decreases and our E_out increases. This constitutes overfitting, so the correct answer is A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3da4a-06e7-4b0d-84eb-b362197f49f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Question 11, Answer C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dc13111b-076c-4628-b4de-2bf237a9dc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can clearly see that the optimal hyperplane is the straight line that occurs at z1 = 0.5\n",
      "Since we know that w1z1 + w2z2 + b = 0 when (z1, z2) fall on the hyperplane, we can derive w1/2 + w2z2 + b = 0\n",
      "Furthermore, since we know that the the weight vector must be perpendicular to the hyperplane (SVM theory), the vector component in the z2 direction must be 0\n",
      "That is to say, w2 = 0\n",
      "So now we have w1/2 + b = 0, or w1 = -2b\n",
      "The only answer choice that satisfies this constraint is C\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#given data\n",
    "X = np.array([[1,  0],\n",
    "             [0,  1],\n",
    "             [0, -1], \n",
    "             [-1, 0], \n",
    "             [0,  2], \n",
    "             [0, -2], \n",
    "             [-2, 0]])\n",
    "y = np.array([-1, -1, -1, 1, 1, 1, 1])\n",
    "\n",
    "def plot(X, y):\n",
    "    # plotting\n",
    "    above = np.where(y > 0)\n",
    "    below = np.where(y < 0)\n",
    "    plt.scatter(X[below,1], X[below,2]) # orange points\n",
    "    plt.scatter(X[above,1], X[above,2]) # blue points\n",
    "    plt.legend([\"-1\", \"+1\"])\n",
    "    plt.xlabel(\"z1\")\n",
    "    plt.ylabel(\"z2\")\n",
    "\n",
    "def transform_11(X):\n",
    "    # applying transform\n",
    "    Z = np.array([(1, ( point[1]**2 -2*point[0] - 1), (point[0]**2 -2*point[1] - 1))for point in X])\n",
    "    return Z\n",
    "\n",
    "#transforming then plotting\n",
    "Z = transform_11(X)\n",
    "plot(Z, y)\n",
    "print('We can clearly see that the optimal hyperplane is the straight line that occurs at z1 = 0.5')\n",
    "print('Since we know that w1z1 + w2z2 + b = 0 when (z1, z2) fall on the hyperplane, we can derive w1/2 + w2z2 + b = 0')\n",
    "print('Furthermore, since we know that the the weight vector must be perpendicular to the hyperplane (SVM theory), the vector component in the z2 direction must be 0')\n",
    "print('That is to say, w2 = 0')\n",
    "print('So now we have w1/2 + b = 0, or w1 = -2b')\n",
    "print('The only answer choice that satisfies this constraint is C')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21049499-2cfa-4c7f-9507-6c8821f02fa8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Question 12, Answer C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "64bc0faf-b202-4eb9-98e2-60f01b3547b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates classification of polynomial kernel for n vs m classification for varying degrees of kernel, and varying regularization constants\n",
    "def svm_classifier():    \n",
    "    # fitting the data\n",
    "    classifier = svm.SVC(kernel='poly', gamma=1, degree=2, coef0=1) # getting configured SVM with polynomial kernel\n",
    "    classifier.fit(X, y)\n",
    "    \n",
    "    # in sample error calc\n",
    "    y_in_predict = classifier.predict(X)\n",
    "    E_in = np.mean(y_in_predict != y)\n",
    "\n",
    "    num_support_vec = classifier.support_vectors_.shape[0]\n",
    "    return num_support_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c473abc1-dad5-4402-abc3-33ceb0873536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 support vectors\n",
      "This matches with answer choice C\n"
     ]
    }
   ],
   "source": [
    "num_support_vec = svm_classifier()\n",
    "print(f'{num_support_vec} support vectors')\n",
    "print('This matches with answer choice C')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8c104-6cfe-4d65-9070-78bd97c185b1",
   "metadata": {},
   "source": [
    "# Question 13, Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "db172583-c4cc-4948-918b-ebc6c9c3059c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023\n"
     ]
    }
   ],
   "source": [
    "def target_function(x):\n",
    "    return np.sign(x[1] - x[0] + 0.25*np.sin(np.pi*x[0]))\n",
    "    \n",
    "def generate_points(N_training_points=100):\n",
    "    X = np.random.uniform(-1, 1, size=(N_training_points, 2))\n",
    "    y = [target_function(x) for x in X]\n",
    "    return X, y\n",
    "\n",
    "def SVM_RBF(gamma):\n",
    "    # dataset\n",
    "    X, y = generate_points()\n",
    "\n",
    "    #rbf kernel with SVM\n",
    "    classifier = svm.SVC(C=1000,kernel=\"rbf\", gamma=gamma)\n",
    "\n",
    "    # fitting SVM\n",
    "    classifier.fit(X, y)\n",
    "\n",
    "    # classifying trainset, calculating in sample performance\n",
    "    y_predicted = classifier.predict(X)\n",
    "    E_in = np.mean(y_predicted != y)\n",
    "    \n",
    "    return E_in\n",
    "\n",
    "def find_closest_center(x, centers):\n",
    "    diff = centers - x\n",
    "    mags = np.array([np.linalg.norm(row) for row in diff])\n",
    "    mindex = np.argmin(mags)\n",
    "    return centers[mindex]\n",
    "    pass\n",
    "    \n",
    "def K_means(K, X):\n",
    "    # random initialization of centers\n",
    "    centers = np.random.uniform(-1, 1, size=(K, 2))\n",
    "    old_centers = np.zeros((K, 2))\n",
    "    while not np.array_equal(np.sort(old_centers), np.sort(centers)):\n",
    "        old_centers = centers\n",
    "        clusters = {} \n",
    "\n",
    "        for x in X:\n",
    "            closest_center = find_closest_center(x, centers)\n",
    "            if clusters.get(tuple(closest_center), None):\n",
    "                clusters[tuple(closest_center)].append(x)\n",
    "            else:\n",
    "                clusters[tuple(closest_center)] = [x]\n",
    "        centers = []\n",
    "        for key, item in clusters.items():\n",
    "            cluster = np.array(item)\n",
    "            new_center = np.mean(cluster, axis=0) # calculate the mean of each columns\n",
    "            centers.append(new_center)\n",
    "        centers = np.array(centers)\n",
    "    return centers\n",
    "\n",
    "def get_phi(X, centers, gamma):\n",
    "    phi = np.array(\n",
    "        [\n",
    "            [np.exp(-1 * gamma * (np.linalg.norm(x - mu) ** 2)) for mu in centers] # each element of a row\n",
    "            for x in X # each row\n",
    "        ]\n",
    "    )\n",
    "    return phi\n",
    "\n",
    "def rbf_reg(phi, y):\n",
    "    phiTphi = np.dot(phi.T, phi)\n",
    "    pinv = np.dot(np.linalg.inv(phiTphi ), phi.T)\n",
    "    w = np.dot(pinv, y) # w = pseudo_inv(X)*y \n",
    "    return w   \n",
    "\n",
    "def rbf_predict_error(w, X_test, y_test, centers, gamma):\n",
    "    phi_test = get_phi(X_test, centers, gamma)\n",
    "    y_predicted = np.sign(phi_test @ w)\n",
    "    return np.mean(y_test != y_predicted)\n",
    "\n",
    "def Kmeans_RBF(K, gamma):\n",
    "    X, y = generate_points()\n",
    "    centers = K_means(K, X) # lloyds algorithm for clustering\n",
    "\n",
    "    phi = get_phi(X, centers, gamma) # phi is a N x K matrix\n",
    "    # pseudo inverse\n",
    "    w = rbf_reg(phi, y)\n",
    "\n",
    "    X_test, y_test = generate_points(1000)#1000 test points\n",
    "    E_in = rbf_predict_error(w, X, y, centers, gamma)\n",
    "    E_out = rbf_predict_error(w, X_test, y_test, centers, gamma)\n",
    "    return E_in, E_out\n",
    "\n",
    "Kmeans_RBF(6, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "420e120f-4ae9-443d-b5c5-cdc6ceabc663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7% of the time, the data is inseperable by the RBF kernel\n",
      "The correct answer is thus A\n"
     ]
    }
   ],
   "source": [
    "E_in_list = []\n",
    "for i in range(1000):\n",
    "    E_in_list.append(SVM_RBF(gamma=1.5))\n",
    "non_seperable_count = len([E_in for E_in in E_in_list if E_in > 0])\n",
    "print(f'{non_seperable_count/1000 * 100}% of the time, the data is inseperable by the RBF kernel')\n",
    "print('The correct answer is thus A')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
