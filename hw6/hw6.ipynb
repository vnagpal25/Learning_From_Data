{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46daf0ee",
   "metadata": {},
   "source": [
    "# Question 1, Answer B\n",
    "###### We know that deterministic noise is a directly related to the complexity of our target function and the ability of the 'best' hypothesis from our Hypotheis Set to approximate that target. It is intuitive to see that as we choose more complex hypothesis sets, the ability for our 'best' hypothesis to approximate the target function increases, therefore our deterministic noise decreases.\n",
    "\n",
    "###### Now, we know that $H' \\subset H$, therefore $H'$ is less complex than $H$. Therefore we know that the best hypothesis from $H'$ will not be able to approximate the target function as well as the best hypothesis from $H$. Therefore the deterministic noise increases if use $H'$ instead of $H$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e610d",
   "metadata": {},
   "source": [
    "# Question 2, Answer A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c4580ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(file_name):\n",
    "    data = np.genfromtxt(file_name)\n",
    "    return data[:, :2], data[:, 2] # X in first two columns, y in last column\n",
    "\n",
    "def non_lin_trans(X):\n",
    "    Z = np.array([(1, point[0], point[1], point[0] ** 2, point[1] ** 2, \n",
    "                   point[0] * point[1], np.absolute(point[0] - point[1]), np.absolute(point[0] + point[1])) for point in X])\n",
    "    return Z\n",
    "\n",
    "def run_linear_regression(Z, y):\n",
    "    ZtZ = np.dot(Z.T, Z)\n",
    "    pinv_Z = np.dot(np.linalg.inv(ZtZ), Z.T)\n",
    "    w = np.dot(pinv_Z, y) # w = pseudo_inv(X)*y \n",
    "    return w\n",
    "\n",
    "def compute_error(w, Z, y):\n",
    "    #for each 'point'/row z in Z, compute wTz and compare sign(wTz) with y\n",
    "     # will contain 0 or 1, for correct/incorrect classification\n",
    "    error_list = [(np.sign(w.T @ z) != y_point) for z, y_point in zip(Z, y)]\n",
    "    return np.sum(error_list)/len(error_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d83c95bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The in-sample error is 0.03, while the out of sample error is 0.08\n",
      "The correct answer choice is A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, y_train = load_data('in.dta')\n",
    "Z_train = non_lin_trans(X_train)# transform data using non linear transformation\n",
    "w_lin = run_linear_regression(Z_train, y_train)# one step learning using linear regression\n",
    "E_in = compute_error(w_lin, Z_train, y_train)\n",
    "\n",
    "X_test, y_test = load_data('out.dta')\n",
    "Z_test = non_lin_trans(X_test)# transform data using non linear transformation\n",
    "E_out = compute_error(w_lin, Z_test, y_test)\n",
    "\n",
    "print(f'The in-sample error is {\"{:.2f}\".format(E_in)}, while the out of sample error is {\"{:.2f}\".format(E_out)}')\n",
    "print('The correct answer choice is A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c40bb",
   "metadata": {},
   "source": [
    "# Question 3, Answer D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bd8576ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function that takes k as a parameter\n",
    "def run_linear_regression(Z, y, lamda):\n",
    "    _, cols = np.shape(Z)\n",
    "    ZtZ = np.dot(Z.T, Z)\n",
    "    pinv_Z = np.dot(np.linalg.inv(ZtZ + lamda * np.identity(cols)), Z.T)\n",
    "    w = np.dot(pinv_Z, y) # w = pseudo_inv(X)*y \n",
    "    return w    \n",
    "\n",
    "def regularize_lin_reg(k):\n",
    "    lamda = 10 ** k # purposely misspelling lambda because it is a keyword in python\n",
    "    X_train, y_train = load_data('in.dta')\n",
    "    \n",
    "    Z_train = non_lin_trans(X_train)# transform data using non linear transformation\n",
    "    w_lin = run_linear_regression(Z_train, y_train, lamda)# one step learning using linear regression\n",
    "    E_in = compute_error(w_lin, Z_train, y_train)\n",
    "\n",
    "    X_test, y_test = load_data('out.dta')\n",
    "    Z_test = non_lin_trans(X_test)# transform data using non linear transformation\n",
    "    E_out = compute_error(w_lin, Z_test, y_test)\n",
    "    return E_in, E_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44e99111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The in-sample error is 0.03, while the out of sample error is 0.08\n",
      "The correct answer choice is D\n"
     ]
    }
   ],
   "source": [
    "E_in, E_out = regularize_lin_reg(k=-3)\n",
    "print(f'The in-sample error is {\"{:.2f}\".format(E_in)}, while the out of sample error is {\"{:.2f}\".format(E_out)}')\n",
    "print('The correct answer choice is D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b758a",
   "metadata": {},
   "source": [
    "# Question 4, Answer E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2efa09e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The in-sample error is 0.4, while the out of sample error is 0.4\n",
      "The correct answer choice is E\n"
     ]
    }
   ],
   "source": [
    "E_in, E_out = regularize_lin_reg(k=3)\n",
    "print(f'The in-sample error is {\"{:.1f}\".format(E_in)}, while the out of sample error is {\"{:.1f}\".format(E_out)}')\n",
    "print('The correct answer choice is E')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b80994",
   "metadata": {},
   "source": [
    "# Question 5, Answer D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "60d26f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error list: [0.084, 0.056, 0.092, 0.124, 0.228]\n",
      "List of k: [-2, -1, 0, 1, 2]\n",
      "As we can see the minimum out of sample error is E_out = 0.056 for k = -1\n",
      "The correct answer choice is D\n"
     ]
    }
   ],
   "source": [
    "k_s = range(-2, 3)\n",
    "E_list = [regularize_lin_reg(k) for k in k_s]\n",
    "E_out_list = [E[1] for E in E_list]\n",
    "\n",
    "\n",
    "print(f'Error list: {E_out_list}')\n",
    "print(f'List of k: {list(k_s)}')\n",
    "print(f'As we can see the minimum out of sample error is E_out = {E_out_list[1]} for k = {k_s[1]}')\n",
    "print('The correct answer choice is D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f6ea4",
   "metadata": {},
   "source": [
    "# Question 6, Answer B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c84f816c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum out of sample error: 0.06\n",
      "The correct answer choice is B\n"
     ]
    }
   ],
   "source": [
    "print(f'Minimum out of sample error: {\"{:.2f}\".format(min(E_out_list))}')\n",
    "print('The correct answer choice is B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac5434",
   "metadata": {},
   "source": [
    "# Question 7, Answer C\n",
    "##### According to the definition of $H(Q, C, Q_0)$, $H(10, 0, 3)$ will have weights vector $w$ of the form $w = (w_1, w_2, 0, 0, 0, 0, 0, 0, 0, 0)$\n",
    "##### This is equivalent to a weights vector from the hypothesis set $H_2$, which will be of the form $w = (w_1, w_2)$\n",
    "##### Similarly, $H(10, 0, 4)$ will have weights vector $w$ of the form $w = (w_1, w_2, w_3, 0, 0, 0, 0, 0, 0, 0)$\n",
    "##### This is equivalent to a weights vector from the hypothesis set $H_3$, which will be of the form $w = (w_1, w_2, w_3)$\n",
    "\n",
    "##### We can see that any vector in $H_2$ can be generated by $H_4$ by letting $w_3=0$. Therefore, $H_2$ is a subset of $H_3$\n",
    "\n",
    "##### We can not make any generalizations of vectors from sets $H(10, 1, 3)$ or $H(10, 1, 4)$, except that the former set is a subset of the latter set:\n",
    "\n",
    "##### Let $w_a \\epsilon H(10, 1, 3)$. $w_a$ can thus be written as $w_a = (w_1, w_2, 1, 1, 1, 1, 1, 1, 1, 1)$\n",
    "##### Let $w_b \\epsilon H(10, 1, 4)$. $w_b$ can thus be written as $w_b = (w_1, w_2, w_3, 1, 1, 1, 1, 1, 1, 1)$\n",
    "##### $H(10, 1, 3) \\subset H(10, 1, 4)$, however, we can't make any generalizations regarding either of them being related to the definition of $H_Q$\n",
    "\n",
    "##### a. $H_2 \\cup H_3 = H_3$ Since $H_3$ is a superset, so this choice is not correct\n",
    "##### b. as stated earlier, we can't make any generalizations regarding definition of $H_Q$ here\n",
    "##### c. $H_2 \\cap H_3 = H_2$, this is correct\n",
    "##### d. can't make any generalizations again\n",
    "\n",
    "##### Therefore, the correct answer choice is C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca66da",
   "metadata": {},
   "source": [
    "# Question 8, Answer D\n",
    "##### In forward propogation of a fully connected neural network, each of the nodes in a layer contributes to the activation of each node in a consecutive next layer. In the first layer, there are 6 nodes, in the second there are 4 nodes(3 of which are activated), and in the last layer there is one node.\n",
    "\n",
    "##### Therefore, from the first layer to the second there are $6*3 = 18$ calculations of the form $w_{ij}^{(l)}x^{(l-1)}_i$ performed. From the second to the third layer, there are $4 * 1 = 4$ calculations performed. So in total 22 of these calculations were done.\n",
    "\n",
    "##### In backpropogation, to compute the sensitivity vector of layer l for each node, we must do an operation that involves taking the dot product of the weight vector with the sensitivity vector of l + 1. We only compute the sensitivity for layers l, l-1, .., 1. Therefore, here we only have to take this dot product once. There are 3 weights in the second layer, for which we compute the sensitivity. Therefore, we perform the $w_{ij}^{(l)}\\delta^{(l)}_j$ operation 3 times.\n",
    "\n",
    "##### Now we compute the necessary partial derivatives for each node. Since there are 4 weights in the second layer, we compute the operation $x^{(l-1)}_i\\delta^{(l)}_j$. In the first layer there are 18 weights, so the operation is performed 18 times. In total, it is performed 22 times.\n",
    "\n",
    "##### $22 + 22 + 3 = 47 \\approx 45$ \n",
    "##### Therefore, the correct answer choice here is D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c17d85",
   "metadata": {},
   "source": [
    "# Question 9, Answer A\n",
    "##### The minimum possible combination should have 1 node per hidden layer + 1 bias, because then there is only (number of nodes in previous layer) weights.\n",
    "##### So if there are 36 nodes in the hidden layers, then there should be 18 hidden layers. For the first hidden layer, there are 10 weights, because of the 10 inputs.\n",
    "##### Each of the subsequent 18 layers(including final output) has 36 weights.\n",
    "##### Therefore, there 10 + 18*2 = 46 weights in a neural network with the given constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd32e166",
   "metadata": {},
   "source": [
    "# Question 10, Answer E\n",
    "##### This is a combinatorial problem. So let's exhaust the possibilities.\n",
    "##### For simplicity, let's try just two hidden layers. My intuition is that a multiplication between 2 2 digit numbers will be significant enough to reach a max in 2 hidden layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b7b9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Architecture\n",
      "(10, 2, 34, 1) 110\n",
      "(10, 3, 33, 1) 149\n",
      "(10, 4, 32, 1) 186\n",
      "(10, 5, 31, 1) 221\n",
      "(10, 6, 30, 1) 254\n",
      "(10, 7, 29, 1) 285\n",
      "(10, 8, 28, 1) 314\n",
      "(10, 9, 27, 1) 341\n",
      "(10, 10, 26, 1) 366\n",
      "(10, 11, 25, 1) 389\n",
      "(10, 12, 24, 1) 410\n",
      "(10, 13, 23, 1) 429\n",
      "(10, 14, 22, 1) 446\n",
      "(10, 15, 21, 1) 461\n",
      "(10, 16, 20, 1) 474\n",
      "(10, 17, 19, 1) 485\n",
      "(10, 18, 18, 1) 494\n",
      "(10, 19, 17, 1) 501\n",
      "(10, 20, 16, 1) 506\n",
      "(10, 21, 15, 1) 509\n",
      "(10, 22, 14, 1) 510\n",
      "(10, 23, 13, 1) 509\n",
      "(10, 24, 12, 1) 506\n",
      "(10, 25, 11, 1) 501\n",
      "(10, 26, 10, 1) 494\n",
      "(10, 27, 9, 1) 485\n",
      "(10, 28, 8, 1) 474\n",
      "(10, 29, 7, 1) 461\n",
      "(10, 30, 6, 1) 446\n",
      "(10, 31, 5, 1) 429\n",
      "(10, 32, 4, 1) 410\n",
      "(10, 33, 3, 1) 389\n",
      "As we can see, the maximum occurs at the network layout with 22 nodes in hidden layer 1 and 14 nodes in hidden layer 2, with 510 total weights\n",
      "Therefore, the correct answer choice is E\n"
     ]
    }
   ],
   "source": [
    "hidden_units = 36\n",
    "#there has to be a minimum of 2 units(1 node, 1 bias node) and a maximum of 34\n",
    "#because that leaves 2 for layer 2(again 1 bias and 1 node)\n",
    "\n",
    "num_weights_list = {}\n",
    "for l1_nodes in range(2, 34):\n",
    "    # the leftover nodes have to be used in the 3rd layer\n",
    "    l2_nodes = hidden_units - l1_nodes\n",
    "    num_weights = 0\n",
    "    num_weights += (l1_nodes-1)*10 # 10 nodes in input layer, -1 because bias doesn't need activation\n",
    "    num_weights += (l2_nodes-1)*l1_nodes # same logic\n",
    "    num_weights += 1 * l2_nodes # 1 node in final layer\n",
    "    num_weights_list[(10, l1_nodes,l2_nodes, 1)] = num_weights\n",
    "print('Network Architecture')\n",
    "[print(architecture, num_weights_list[architecture])for architecture in num_weights_list]\n",
    "print(f'As we can see, the maximum occurs at the network layout with 22 nodes in hidden layer 1 and 14 nodes in hidden layer 2, with 510 total weights')\n",
    "print('Therefore, the correct answer choice is E')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
